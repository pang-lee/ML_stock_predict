# coding: utf-8
import pandas as pd
from datetime import datetime, timedelta
from dotenv import load_dotenv
load_dotenv()
import os
import shioaji as sj
from fugle_marketdata import WebSocketClient, RestClient
import yfinance as yf
import seaborn as sns
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup
from statsmodels.tsa.stattools import coint, adfuller
import numpy as np

# https://ithelp.ithome.com.tw/articles/10280898 æ•™å­¸åƒè€ƒ
def get_shioaji_index_data(begin, end, id, type='1k'):
    # å°‡ begin å’Œ end è½‰æ›ç‚ºæ‰€éœ€çš„æ ¼å¼ 2024_0708
    begin_month = begin[:7].replace('-', '')  # å–å¾—2024-07éƒ¨åˆ†ï¼Œè®Šæˆ202407
    end_month = end[:7].replace('-', '')      # å–å¾—2024-08éƒ¨åˆ†ï¼Œè®Šæˆ202408
    folder_name = f"{begin[:4]}_{begin_month[4:6]}{end_month[4:6]}"  # 2024_0708
    
    # å®šç¾©è·¯å¾‘
    folder_path = f"./index_data/shioaji/{folder_name}/"
    
    # å¦‚æœè³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œå‰‡å‰µå»º
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
        
    # å®šç¾©CSVæª”æ¡ˆçš„åç¨±
    file_name = f"{id}.csv"
    file_path = os.path.join(folder_path, file_name)
    
    api = sj.Shioaji()
    api.login(
        api_key=os.getenv('API_KEY'),
        secret_key=os.getenv('SECRET_KEY'),
        fetch_contract=False,
    )
    api.fetch_contracts(contract_download=True)
    #å¦‚æœè¦æŸ¥çœ‹åˆç´„ä»£è™Ÿå¯ä»¥ç›´æ¥print(api.Contracts), æŸ¥çœ‹ç‰¹å®šçš„è‚¡ç¥¨å¯ä»¥print(api.Contracts.Stocks.TSE['2330'])
    
    # å‰µå»ºä¸€å€‹ç©ºçš„ DataFrame ä¾†å­˜å„²æ‰€æœ‰æ•¸æ“š
    all_ticks = pd.DataFrame()

    if type == '1k':
        print('Remain usage API usage:', api.usage())
            
        # èª¿ç”¨ API ç²å–ç•¶å¤©çš„æ•¸æ“š
        kbars = api.kbars(
            contract=api.Contracts.Stocks.TSE[id],
            start=begin,
            end=end,
        )
            
        df = pd.DataFrame({**kbars})
        all_ticks = pd.concat([all_ticks, df], ignore_index=True)
        
        print(f"Processed data from {begin} to {end}")
        
        all_ticks['ts'] = pd.to_datetime(all_ticks['ts'])

        all_ticks.to_csv(file_path, index=False)
        print(f'Data saved to {file_path}')
        
    elif type == 'tick':
        # å°‡ begin å’Œ end è½‰æ›ç‚º datetime å°è±¡
        start_date = datetime.strptime(begin, "%Y-%m-%d")
        end_date = datetime.strptime(end, "%Y-%m-%d")
    
        # å¾ªç’°æ—¥æœŸç¯„åœ
        current_date = start_date
        while current_date <= end_date:
            date_str = current_date.strftime("%Y-%m-%d")
            print(f"Fetching data for {date_str}..., Remain usage API usage: {api.usage()}")
            
            try:
                # èª¿ç”¨ API ç²å–ç•¶æ—¥æ•¸æ“š
                ticks = api.ticks(
                    contract=api.Contracts.Stocks[id],
                    date=date_str
                )
                df = pd.DataFrame({**ticks})
            
                if not df.empty:
                    # åˆä½µç•¶å¤©æ•¸æ“šåˆ°ç¸½è¡¨
                    all_ticks = pd.concat([all_ticks, df], ignore_index=True)
                    print(f"Data for {date_str} processed.")
                else:
                    print(f"No data available for {date_str}.")
        
            except Exception as e:
                print(f"Error fetching data for {date_str}: {e}")
        
            # æ—¥æœŸå¢åŠ ä¸€å¤©
            current_date += timedelta(days=1)   
        
        all_ticks['ts'] = pd.to_datetime(all_ticks['ts'])
        all_ticks.to_csv(file_path, index=False)
        print(f"Data saved to {file_path}")
        
    elif type == 'future-k':
        # å¾ªç’°æ—¥æœŸç¯„åœ
        print(f"Fetching data for {begin}, {end}..., Remain usage API usage: {api.usage()}")
        
        try:
            # èª¿ç”¨ API ç²å–ç•¶æ—¥æ•¸æ“š
            ticks = api.kbars(
                contract=api.Contracts.Futures.TXF[id],
                start=begin,
                end=end
            )
            df = pd.DataFrame({**ticks})
            
            if not df.empty:
                # åˆä½µç•¶å¤©æ•¸æ“šåˆ°ç¸½è¡¨
                all_ticks = pd.concat([all_ticks, df], ignore_index=True)
                print(f"Data for {begin}, {end} processed.")
            else:
                print(f"No data available for {begin}, {end}.")
        
        except Exception as e:
            print(f"Error fetching data for {begin}, {end}: {e}")
        
        all_ticks['ts'] = pd.to_datetime(all_ticks['ts'])
        all_ticks.to_csv(file_path, index=False)
        print(f"Data saved to {file_path}")
   
    elif type == 'future-t':
        # å°‡ begin å’Œ end è½‰æ›ç‚º datetime å°è±¡
        start_date = datetime.strptime(begin, "%Y-%m-%d")
        end_date = datetime.strptime(end, "%Y-%m-%d")
    
        # å¾ªç’°æ—¥æœŸç¯„åœ
        current_date = start_date
        while current_date <= end_date:
            date_str = current_date.strftime("%Y-%m-%d")
            print(f"Fetching data for {date_str}..., Remain usage API usage: {api.usage()}")
            
            try:
                # èª¿ç”¨ API ç²å–ç•¶æ—¥æ•¸æ“š
                ticks = api.ticks(
                    contract=api.Contracts.Futures[id],
                    date=date_str
                )
                df = pd.DataFrame({**ticks})
            
                if not df.empty:
                    # åˆä½µç•¶å¤©æ•¸æ“šåˆ°ç¸½è¡¨
                    all_ticks = pd.concat([all_ticks, df], ignore_index=True)
                    print(f"Data for {date_str} processed.")
                else:
                    print(f"No data available for {date_str}.")
        
            except Exception as e:
                print(f"Error fetching data for {date_str}: {e}")
        
            # æ—¥æœŸå¢åŠ ä¸€å¤©
            current_date += timedelta(days=1)
        
        all_ticks['ts'] = pd.to_datetime(all_ticks['ts'])
        all_ticks.to_csv(file_path, index=False)
        print(f"Data saved to {file_path}")

    elif type == 'index':
        # è¨­ç½®é–‹å§‹æ—¥æœŸå’ŒçµæŸæ—¥æœŸ
        start_date, end_date = datetime.strptime(begin, "%Y-%m-%d"), datetime.strptime(end, "%Y-%m-%d")
        
        # éæ­·æ—¥æœŸç¯„åœï¼ŒæŒ‰å¤©èª¿ç”¨ API
        current_date = start_date
        
        while current_date <= end_date:
            print('Remain usage API usage:', api.usage())

            # æ ¼å¼åŒ–æ—¥æœŸç‚ºå­—ç¬¦ä¸²
            date_str = current_date.strftime("%Y-%m-%d")

            # èª¿ç”¨ API ç²å–ç•¶å¤©çš„æ•¸æ“š TSE001(å¤§ç›¤), TSE099(å°ç£50)
            ticks = api.ticks(
                contract=api.Contracts.Indexs.TSE.TSE001,
                date=date_str
            )

            df = pd.DataFrame({**ticks})
            all_ticks = pd.concat([all_ticks, df], ignore_index=True)

            print(f"Processed data for {date_str}")

            current_date += timedelta(days=1)

        all_ticks = all_ticks[['ts', 'close']]
        all_ticks['ts'] = pd.to_datetime(all_ticks['ts'])

        all_ticks.to_csv(file_path, index=False)
        print(f'Data saved to {file_path}')

# get_shioaji_index_data('2024-12-02', '2025-01-02', '2353', type='tick')
# get_shioaji_index_data('2024-06-01', '2025-01-22', '00631L', type='tick')
# get_shioaji_index_data('2024-06-01', '2025-01-22', 'TXFR1', type='future-k')
# get_shioaji_index_data('2025-04-01', '2025-04-11', 'MXFR1', type='future-t')
get_shioaji_index_data('2025-04-21', '2025-04-28', 'TMFR1', type='future-t')


#ä½¿ç”¨Fugle APIç²å¾—è³‡æ–™
def get_fugle_data(stock_list, data_folder):
    folder = f'./index_data/fugle/{data_folder}'

    client = RestClient(api_key = os.getenv('FUGLE'))
    
    # éå†è‚¡ç¥¨ä»£ç åˆ—è¡¨
    for stock_code in stock_list:
        stock = client.stock.historical.candles(**{"symbol": f'{stock_code}',  "timeframe": 1})
    
        if 'symbol' not in stock:
            raise ValueError(stock['message'])

        data = pd.DataFrame(stock['data'])
        data.rename(columns={'date': 'datetime'}, inplace=True)
        data['datetime'] = pd.to_datetime(data['datetime']).dt.strftime('%Y-%m-%d %H:%M:%S')
        data.set_index('datetime', inplace=True)
        reversed_df = data.iloc[::-1]
        reversed_df.to_csv(f'{folder}/{data_folder}/{stock_code}.csv')
        
    print(f'Data All safet to {folder}/{data_folder}')

# ç²å–ETF
stock_list = ["00631L", "00632R", "00663L", "00664R", "00675L", "00676R", "00685L", "00686R", 
              "00633L", "00634R", "00637L", "00638R", "00655L", "00656R", "00753L", "00650L",
              "00651R", "00665L", "00666R", "00640L", "00641R", "00654R", "00647L", "00648R",
              "00852L", "00669R", "00670L", "00671R", "00683L", "00684R", "00706L", "00707R",
              "00708L", "00674R", "00673R", "00715L", "00680L", "00681R", "00688L", "00689R",
              "2330"
            ]


#æ¯å€‹æœˆè¦æ›´æ›ä¸€æ¬¡æœˆä»½ä»£è™Ÿ(2024_0708 => 2024_0809)
# get_fugle_data(stock_list=stock_list, data_folder='2024_0809')


# ç²å–ç†±åŠ›åœ–
def get_stock_data(stock_list, start, end):
    data = {}
    for stock in stock_list:
        print(f"Attempting to fetch {stock} (TW)")
        stock_data = yf.download(stock, start=start, end=end, auto_adjust=True)[['Close', 'Volume']]
        
        if stock_data.empty:
            print(f"Data for {stock} (TW) is empty, retrying with .TWO")
            new_stock = stock.replace('.TW', '.TWO')
            stock_data = yf.download(new_stock, start=start, end=end, auto_adjust=True)[['Close', 'Volume']]
            
            if stock_data.empty:
                print(f"Failed to fetch data for {new_stock} (TWO), skipping.")
                continue
            
        data[stock] = stock_data
    return data

def get_heatmap(html, stock_ids, start, end, type=1, idx_start=0, idx_end=-1):
    if html and not stock_ids:
        soup = BeautifulSoup(html, 'html.parser')
        if type == 1:  # ç²å–ä¸€èˆ¬çš„é¸è‚¡ç¶²é 
            stock_ids = [span.text for span in soup.find_all('span', {'c-model': 'id'})]
        elif type == 2:  # ç²å–æˆäº¤å€¼ç¶²é 
            a_tags = soup.find_all('a')
            stock_ids = [a['href'].split('/')[-1] for a in a_tags if 'href' in a.attrs and a['href'].startswith('/stock/')]

        if idx_end == -1 or idx_end > len(stock_ids):
            idx_end = len(stock_ids)
        stock_ids = stock_ids[idx_start:idx_end]

    stock_list = [stock_id + '.TW' for stock_id in stock_ids]
    # ä½¿ç”¨get_stock_dataå‡½æ•¸ä¸‹è¼‰æ•¸æ“šï¼ˆåŒ…å«æ”¶ç›¤åƒ¹èˆ‡æˆäº¤é‡ï¼‰
    raw_data = get_stock_data(stock_list, start, end)
    
    # å°‡æ”¶ç›¤åƒ¹èˆ‡æˆäº¤é‡æ‹†æˆå…©å€‹ DataFrame
    close_data = {k: v['Close'] for k, v in raw_data.items()}
    
    # ç§»é™¤ç¼ºå¤±æ•¸æ“š
    close_data = {k: v for k, v in close_data.items() if not v.isnull().all().all()}
    close_data = {k: v.squeeze() for k, v in close_data.items()}

    if not close_data:
        print("No valid stock data found.")
        return

    # å°‡æ”¶ç›¤åƒ¹è½‰æ›ç‚º DataFrame
    close_data = pd.DataFrame(close_data)

    # åˆå§‹åŒ–å”æ•´çŸ©é™£èˆ‡ ADF å¹³ç©©æ€§çµæœ
    num_stocks = len(close_data.columns)
    coint_matrix = np.ones((num_stocks, num_stocks))
    adf_results = {}
    mean_reverting_stocks = []
    cointegrated_pairs = []

    # é€²è¡ŒADFæª¢é©—
    for stock in close_data.columns:
        adf_pvalue = adfuller(close_data[stock].dropna())[1]
        adf_results[stock] = adf_pvalue
        if adf_pvalue <= 0.05:
            mean_reverting_stocks.append(stock)

    # è¨ˆç®—å”æ•´æª¢å®š (åƒ…åœ¨ ADF ä¸é¡¯è‘—æ™‚é€²è¡Œ)
    for i in range(num_stocks):
        for j in range(i + 1, num_stocks):
            stock1, stock2 = close_data.columns[i], close_data.columns[j]
            stock1_series = close_data[stock1].dropna()
            stock2_series = close_data[stock2].dropna()
            
            # å¦‚æœä»»ä¸€è‚¡ç¥¨å¹³ç©©ï¼Œå‰‡è·³éå”æ•´æª¢å®š
            if adf_results[stock1] <= 0.05 or adf_results[stock2] <= 0.05:
                continue

            # å”æ•´æª¢å®š
            common_index = stock1_series.index.intersection(stock2_series.index)
            if len(common_index) > 1:
                score, p_value, _ = coint(stock1_series[common_index], stock2_series[common_index])
                coint_matrix[i, j] = p_value
                coint_matrix[j, i] = p_value

                # å¦‚æœå”æ•´æª¢å®šé€šéï¼Œä¸¦ä¸”éƒ½æ˜¯éå¹³ç©©çš„ï¼Œè¨˜éŒ„ä¸‹çµ±è¨ˆå¥—åˆ©çš„æ¨™çš„å°
                if p_value <= 0.05:
                    cointegrated_pairs.append((stock1, stock2, p_value))

    # å°‡å”æ•´çµæœè½‰ç‚º DataFrame ä¸¦éæ¿¾
    coint_df = pd.DataFrame(coint_matrix, index=close_data.columns, columns=close_data.columns)
    coint_df = coint_df.where(coint_df <= 0.05)  # åªä¿ç•™é€šéæª¢å®šçš„å”æ•´é—œä¿‚

    # å¯è¦–åŒ–ç†±åŠ›åœ–
    plt.figure(figsize=(10, 6))
    sns.heatmap(coint_df, annot=True, cmap='coolwarm', linewidths=0.5)
    plt.title('Cointegration Heatmap (p-value)')
    plt.xticks(rotation=45)
    plt.show()

    # åœ¨é€™è£¡å°‡æˆäº¤é‡ã€æ”¶ç›¤åƒ¹ï¼ˆæœ€å¾Œä¸€ç­†ï¼‰èˆ‡å¹³å‡æˆäº¤é‡ä¸€ä½µåˆ—å°
    print("\n====== âœ… å¯ä»¥å–®ç¨ä½¿ç”¨å‡å€¼å›æ­¸äº¤æ˜“çš„æ¨™çš„ (å¹³ç©©åºåˆ—) âœ… ======")
    for stock in mean_reverting_stocks:
        last_close = close_data[stock].iloc[-1]  # æœ€å¾Œä¸€ç­†æ”¶ç›¤åƒ¹
        print(f"{stock} (p-value={adf_results[stock]:.4f}, Last Close={last_close:.2f})")

    print("\n====== ğŸ“ˆ é©åˆçµ±è¨ˆå¥—åˆ©çš„æ¨™çš„å° (éå¹³ç©©ä½†å”æ•´) ğŸ“ˆ ======")
    for pair in cointegrated_pairs:
        stock1, stock2, p_value = pair
        last_close1 = close_data[pair[0]].iloc[-1]
        last_close2 = close_data[pair[1]].iloc[-1]
        print(f"{stock1} (Last Close={last_close1:.2f}) & {stock2} (Last Close={last_close2:.2f}), Cointegration p-value={p_value:.4f}")
        print('------------------')

    # è¿”å›ç¯©é¸çµæœä»¥ä¾›é€²ä¸€æ­¥åˆ†æ
    return {
        "mean_reverting_stocks": mean_reverting_stocks,
        "cointegrated_pairs": cointegrated_pairs
    }

# åˆ°ç©è‚¡ç¶²ä¸­ https://www.wantgoo.com/index/overview , æŒ‘é¸é¸è‚¡æ¸…å–®å¾Œ, è¤‡è£½main > container > table (å¤–éƒ¨è¤‡è£½)
h = '''



'''

stock_list = []

# å–tableçš„æ‰€æœ‰è³‡æ–™
# type 1: ä¸€èˆ¬é¡è‚¡ç¶²é (https://www.wantgoo.com/index/overview#index-7)
# type 2: æœ‰å–®ç¨çš„ä»£è™Ÿçš„ç¶²é (https://www.wantgoo.com/stock/ranking/turnover)
# get_heatmap(h, stock_list, '2024-01-01', '2025-03-11', type=2, idx_start=120, idx_end=160)
